import langchain_groq
import streamlit as st
import os
from groq import Groq
import random
import nltk
from nltk import pos_tag, word_tokenize
from langchain.chains import ConversationChain, LLMChain
from langchain_core.prompts import (
    ChatPromptTemplate,
    HumanMessagePromptTemplate,
    MessagesPlaceholder,
)
from langchain_core.messages import SystemMessage
from langchain.chains.conversation.memory import ConversationBufferWindowMemory
from langchain_groq import ChatGroq
from langchain.prompts import PromptTemplate

# Download the NLTK data files
nltk.download('punkt')
nltk.download('averaged_perceptron_tagger')


def get_grammar_info(word):
    """
    Takes a word as input and returns its part of speech and any grammar-related information.
    """
    tokens = word_tokenize(word)
    pos_tags = pos_tag(tokens)

    # This will return a list of tuples (word, part_of_speech)
    return pos_tags


def main():
    """
    This function is the main entry point of the application. It sets up the Groq client, the Streamlit interface, and handles the chat interaction.
    """

    # Get Groq API key
    api_key = os.getenv("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")

    client = Groq(api_key="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")

    # Display the Groq logo
    spacer, col = st.columns([5, 1])

    # The title and greeting message of the Streamlit application
    st.title("AI-Enhanced Vocabulary and Grammar Assistant!")
    st.write(
        "Hello! I'm your friendly Groq chatbot. I can help answer your questions, provide grammar insights, or just chat. Discover, Learn, and Master New Words Effortlessly! Let's start our conversation.")

    # Add customization options to the sidebar
    st.sidebar.title('Customization')
    system_prompt = st.sidebar.text_input("System prompt:")
    model = st.sidebar.selectbox(
        'Choose a model',
        ['llama3-8b-8192', 'mixtral-8x7b-32768', 'gemma-7b-it']
    )
    conversational_memory_length = st.sidebar.slider('Conversational memory length:', 1, 10, value=5)

    memory = ConversationBufferWindowMemory(k=conversational_memory_length, memory_key="chat_history",
                                            return_messages=True)

    user_question = st.text_input("Enter a word to get its grammar details:")

    # session state variable
    if 'chat_history' not in st.session_state:
        st.session_state.chat_history = []
    else:
        for message in st.session_state.chat_history:
            memory.save_context(
                {'input': message['human']},
                {'output': message['AI']}
            )

    # Initialize Groq Langchain chat object and conversation
    groq_chat = ChatGroq(
        groq_api_key="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",
        model_name=model
    )

    if user_question:
        # Get grammar-related information for the word
        grammar_info = get_grammar_info(user_question)

        # Construct a chat prompt template using various components
        prompt = ChatPromptTemplate.from_messages(
            [
                SystemMessage(
                    content=system_prompt
                ),  # This is the persistent system prompt that is always included at the start of the chat.

                MessagesPlaceholder(
                    variable_name="chat_history"
                ),
                # This placeholder will be replaced by the actual chat history during the conversation. It helps in maintaining context.

                HumanMessagePromptTemplate.from_template(
                    "{human_input}"
                ),  # This template is where the user's current input will be injected into the prompt.
            ]
        )

        # Create a conversation chain using the LangChain LLM (Language Learning Model)
        conversation = LLMChain(
            llm=groq_chat,  # The Groq LangChain chat object initialized earlier.
            prompt=prompt,  # The constructed prompt template.
            verbose=True,  # Enables verbose output, which can be useful for debugging.
            memory=memory,  # The conversational memory object that stores and manages the conversation history.
        )

        # The chatbot's answer is generated by sending the full prompt to the Groq API.
        response = conversation.predict(human_input=user_question)
        message = {'human': user_question, 'AI': response}

        # Add grammar information to the response
        grammar_message = f"Grammar Info for '{user_question}': {grammar_info}"
        st.session_state.chat_history.append(message)
        st.write("Chatbot:", response)
        st.write(grammar_message)

    import time

    time.sleep(4)

    print("This message will display first.")
    print()  # Blank line
    time.sleep(1)

    print("Here's a message with a gap below it.")
    print("\n" * 3)  # Adds 3 blank lines
    time.sleep(1)

    print("This message will display after the gaps.")


if __name__ == "__main__":
    main()
